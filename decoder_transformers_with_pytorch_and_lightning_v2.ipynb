{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44edc41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pip\n",
    "try:\n",
    "  __import__(\"lightning\")\n",
    "except ImportError:\n",
    "  pip.main(['install', \"lightning\"])  \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "583012a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = {'what' : 0,\n",
    "               'is' : 1,\n",
    "               'statquest' : 2,\n",
    "               'awesome': 3,\n",
    "               '<EOS>' : 4, ## <EOS> = end of sequence\n",
    "              }\n",
    "id_to_token = dict(map(reversed, token_to_id.items()))\n",
    "\n",
    "inputs = torch.tensor([[token_to_id[\"what\"], ## input #1: what is statquest <EOS> awesome\n",
    "                        token_to_id[\"is\"], \n",
    "                        token_to_id[\"statquest\"], \n",
    "                        token_to_id[\"<EOS>\"],\n",
    "                        token_to_id[\"awesome\"]], \n",
    "                       \n",
    "                       [token_to_id[\"statquest\"], # input #2: statquest is what <EOS> awesome\n",
    "                        token_to_id[\"is\"], \n",
    "                        token_to_id[\"what\"], \n",
    "                        token_to_id[\"<EOS>\"], \n",
    "                        token_to_id[\"awesome\"]]])\n",
    "labels = torch.tensor([[token_to_id[\"is\"], \n",
    "                        token_to_id[\"statquest\"], \n",
    "                        token_to_id[\"<EOS>\"], \n",
    "                        token_to_id[\"awesome\"], \n",
    "                        token_to_id[\"<EOS>\"]],  \n",
    "                       \n",
    "                       [token_to_id[\"is\"], \n",
    "                        token_to_id[\"what\"], \n",
    "                        token_to_id[\"<EOS>\"], \n",
    "                        token_to_id[\"awesome\"], \n",
    "                        token_to_id[\"<EOS>\"]]])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels) \n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dfe898",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoding(nn.Module):\n",
    "    def __init__(self, d_model=2, max_len=6):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n",
    "        embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n",
    "        \n",
    "        div_term = 1/torch.tensor(10000.0)**(embedding_index / d_model)\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, word_embeddings):\n",
    "        return word_embeddings + self.pe[:word_embeddings.size(0), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "450a1b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model=d_model\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        \n",
    "        self.row_dim = 0\n",
    "        self.col_dim = 1\n",
    "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
    "        q = self.W_q(encodings_for_q)\n",
    "        k = self.W_k(encodings_for_k)\n",
    "        v = self.W_v(encodings_for_v)\n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "\n",
    "        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "        \n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "        \n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "498a8359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyTransformer(L.LightningModule):\n",
    "    \n",
    "    def __init__(self, num_tokens=4, d_model=2, max_len=6):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        L.seed_everything(seed=42)\n",
    "        \n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, \n",
    "                               embedding_dim=d_model)     \n",
    "        \n",
    "        self.pe = PositionEncoding(d_model=d_model, \n",
    "                                   max_len=max_len)\n",
    "\n",
    "        self.self_attention = Attention(d_model=d_model)\n",
    "\n",
    "        self.fc_layer = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
    "        \n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "                \n",
    "        word_embeddings = self.we(token_ids)        \n",
    "        position_encoded = self.pe(word_embeddings)\n",
    "        \n",
    "        mask = torch.tril(torch.ones((token_ids.size(dim=0), token_ids.size(dim=0)), device=self.device))\n",
    "        \n",
    "        mask = mask == 0\n",
    "        \n",
    "        self_attention_values = self.self_attention(position_encoded, \n",
    "                                                    position_encoded, \n",
    "                                                    position_encoded, \n",
    "                                                    mask=mask)\n",
    "                \n",
    "        residual_connection_values = position_encoded + self_attention_values\n",
    "        \n",
    "        fc_layer_output = self.fc_layer(residual_connection_values)\n",
    "        \n",
    "        return fc_layer_output\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self): \n",
    "        return Adam(self.parameters(), lr=0.1)\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx): \n",
    "        input_tokens, labels = batch # collect input\n",
    "        output = self.forward(input_tokens[0])\n",
    "        loss = self.loss(output, labels[0])\n",
    "                    \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1a4059e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Tokens:\n",
      "\n",
      "\t <EOS>\n"
     ]
    }
   ],
   "source": [
    "model = DecoderOnlyTransformer(num_tokens=len(token_to_id), d_model=2, max_len=6)\n",
    "\n",
    "model_input = torch.tensor([token_to_id[\"what\"], \n",
    "                            token_to_id[\"is\"], \n",
    "                            token_to_id[\"statquest\"], \n",
    "                            token_to_id[\"<EOS>\"]])\n",
    "input_length = model_input.size(dim=0)\n",
    "\n",
    "predictions = model(model_input) \n",
    "\n",
    "predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "\n",
    "predicted_ids = predicted_id\n",
    "\n",
    "max_length = 6\n",
    "for i in range(input_length, max_length):\n",
    "    if (predicted_id == token_to_id[\"<EOS>\"]):\n",
    "        break\n",
    "    \n",
    "    model_input = torch.cat((model_input, predicted_id))\n",
    "    \n",
    "    predictions = model(model_input) \n",
    "    predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
    "        \n",
    "print(\"Predicted Tokens:\\n\") \n",
    "for id in predicted_ids: \n",
    "    print(\"\\t\", id_to_token[id.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba7bdfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type             | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | we             | Embedding        | 10     | train\n",
      "1 | pe             | PositionEncoding | 0      | train\n",
      "2 | self_attention | Attention        | 12     | train\n",
      "3 | fc_layer       | Linear           | 15     | train\n",
      "4 | loss           | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------------\n",
      "37        Trainable params\n",
      "0         Non-trainable params\n",
      "37        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 92.22it/s, v_num=1] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 61.24it/s, v_num=1]\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=30)\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99178500",
   "metadata": {},
   "source": [
    "Use the Trained Transformer!!!\n",
    "To use the transformer that we just trained, we just repeat what we did earlier, only this time we use the trained transformer instead of an untrained transformer. First, we'll see if it correctly responds to the prompt What is StatQuest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f1b2cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Tokens:\n",
      "\n",
      "\t awesome\n",
      "\t <EOS>\n"
     ]
    }
   ],
   "source": [
    "model_input = torch.tensor([token_to_id[\"what\"], \n",
    "                            token_to_id[\"is\"], \n",
    "                            token_to_id[\"statquest\"], \n",
    "                            token_to_id[\"<EOS>\"]])\n",
    "input_length = model_input.size(dim=0)\n",
    "\n",
    "predictions = model(model_input) \n",
    "predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "predicted_ids = predicted_id\n",
    "\n",
    "for i in range(input_length, max_length):\n",
    "    if (predicted_id == token_to_id[\"<EOS>\"]): # if the prediction is <EOS>, then we are done\n",
    "        break\n",
    "    \n",
    "    model_input = torch.cat((model_input, predicted_id))\n",
    "    \n",
    "    predictions = model(model_input) \n",
    "    predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
    "        \n",
    "print(\"Predicted Tokens:\\n\") \n",
    "for id in predicted_ids: \n",
    "    print(\"\\t\", id_to_token[id.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208c730c",
   "metadata": {},
   "source": [
    "Hooray!!! We got the correct output! Now let's see if it correctly responds to the prompt StatQuest is what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c576ada3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Tokens:\n",
      "\n",
      "\t awesome\n",
      "\t <EOS>\n"
     ]
    }
   ],
   "source": [
    "## Now let's ask the other question...\n",
    "model_input = torch.tensor([token_to_id[\"statquest\"], \n",
    "                            token_to_id[\"is\"], \n",
    "                            token_to_id[\"what\"], \n",
    "                            token_to_id[\"<EOS>\"]])\n",
    "input_length = model_input.size(dim=0)\n",
    "\n",
    "predictions = model(model_input) \n",
    "predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "predicted_ids = predicted_id\n",
    "\n",
    "for i in range(input_length, max_length):\n",
    "    if (predicted_id == token_to_id[\"<EOS>\"]): # if the prediction is <EOS>, then we are done\n",
    "        break\n",
    "    \n",
    "    model_input = torch.cat((model_input, predicted_id))\n",
    "    \n",
    "    predictions = model(model_input) \n",
    "    predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
    "        \n",
    "print(\"Predicted Tokens:\\n\") \n",
    "for id in predicted_ids: \n",
    "    print(\"\\t\", id_to_token[id.item()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
